{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaX_veERW5jF","outputId":"8ce0647f-eb12-4345-9d1d-1e26f5708dda","executionInfo":{"status":"ok","timestamp":1667754253081,"user_tz":-60,"elapsed":32624,"user":{"displayName":"Pablo","userId":"08800897806486834245"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator #Es una clase que nos da keras, tiene metodos que hacen: nos permite recoger imagenes de directorios(mas sencillo), cada categoria un directorio, por cada uno 2 subdirectorios(entreno, validacion)\n","\n","directorio = '/content/drive/MyDrive/Colab Notebooks/Inteligencia Artificial/Datasets/intel'\n","train_datagen = ImageDataGenerator(\n","        rescale=1. / 255,\n","        rotation_range=10, # rotation\n","        width_shift_range=0.2, # horizontal shift\n","        height_shift_range=0.2, # vertical shift\n","        zoom_range=0.2, # zoom\n","        horizontal_flip=True, # horizontal flip\n","        brightness_range=[0.2,1.2]) # brightness\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","\n","train_generator = train_datagen.flow_from_directory(\n","    directorio + '/train',\n","    target_size=(150, 150),#Como el resize\n","    batch_size=20,\n","    class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    directorio + '/validation',\n","    target_size=(150, 150),\n","    batch_size=20,\n","    class_mode='binary')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdAsmYkcXu7u","outputId":"1fa7ed84-b28f-472f-a10e-8363d471bcfb","executionInfo":{"status":"ok","timestamp":1667754316016,"user_tz":-60,"elapsed":30523,"user":{"displayName":"Pablo","userId":"08800897806486834245"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 14038 images belonging to 6 classes.\n","Found 3000 images belonging to 6 classes.\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import Flatten, Dense, Dropout\n","\n","def classification (batch_size = 200, epochs = 20, img_width = 150,    #Si llamo  a la funcion sin argumentos cogera estos por defecto, en otro caso cogera los argumentos especificados y el resto por defecto.\n","                    img_height = 150, num_train_samples = 14038, \n","                    num_validation_samples = 3000):\n","\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu',\n","              input_shape=(img_width, img_height, 3)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(48, (3, 3), activation='relu'))\n","    model.add(Dropout(0.2))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten()) \n","    model.add(Dense(16, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    model.compile(loss='binary_crossentropy', # Una neurona que clasifica perro o gato\n","              optimizer='rmsprop',\n","              metrics=['accuracy'])#Tiene sentido meterlo por q nos da una idea muy intuitiva de lo que estoy haciendo en clasificacion\n","    \n","    print(model.summary())\n","    \n","    history = model.fit(\n","        train_generator,\n","        steps_per_epoch=num_train_samples // batch_size,\n","        epochs=epochs,\n","        validation_data=validation_generator,\n","        validation_steps=num_validation_samples // batch_size, #Los generator sin ules infinitos que t dan valores, entonces de alg8una manera tiene  qsaber cuando cortar keras\n","        verbose =1)\n","    \n","    return history"],"metadata":{"id":"544Wf92VYoq_","executionInfo":{"status":"ok","timestamp":1667755243462,"user_tz":-60,"elapsed":10,"user":{"displayName":"Pablo","userId":"08800897806486834245"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["classification()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":979},"id":"UNJfRnYqZzoQ","outputId":"a492627f-587a-471d-ee04-351867ab6d1a","executionInfo":{"status":"error","timestamp":1667756204038,"user_tz":-60,"elapsed":958532,"user":{"displayName":"Pablo","userId":"08800897806486834245"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_9 (Conv2D)           (None, 148, 148, 32)      896       \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 74, 74, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 72, 72, 48)        13872     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 72, 72, 48)        0         \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 36, 36, 48)       0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 34, 34, 64)        27712     \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 17, 17, 64)       0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_3 (Flatten)         (None, 18496)             0         \n","                                                                 \n"," dense_6 (Dense)             (None, 16)                295952    \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 338,449\n","Trainable params: 338,449\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/20\n","70/70 [==============================] - 865s 12s/step - loss: -4136876.5000 - accuracy: 0.1636 - val_loss: -21566530.0000 - val_accuracy: 0.1300\n","Epoch 2/20\n"," 3/70 [>.............................] - ETA: 10:11 - loss: -19944534.0000 - accuracy: 0.1833"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-320d29d1e9d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-084926aebe7b>\u001b[0m in \u001b[0;36mclassification\u001b[0;34m(batch_size, epochs, img_width, img_height, num_train_samples, num_validation_samples)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_validation_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#Los generator sin ules infinitos que t dan valores, entonces de alg8una manera tiene  qsaber cuando cortar keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         verbose =1)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["#No se que hace, ocultar"],"metadata":{"id":"RWCHbwW_BY66"}},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator#Es una clase que nos da keras, tiene metodos que hacen: nos permite recoger imagenes de directorios(mas sencillo), cada categoria un directorio, por cada uno 2 subdirectorios(entreno, validacion)\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Dropout\n","from keras.layers import Flatten, Dense\n","from keras import regularizers\n","\n","def classification (batch_size = 20, epochs = 20, img_width = 150,\n","                    img_height = 150, \n","                    num_train_samples = 2000, num_validation_samples=800):\n","    train_data_dir = directorio + '/train'\n","    validation_data_dir = directorio + '/validation'\n","\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), activation='relu',\n","              input_shape=(img_width, img_height, 3)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(48, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Conv2D(64, (3, 3), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    #model.add(Dropout(0.4))      # Dropout\n","    model.add(Flatten())\n","    model.add(Dense(64, activation='relu',\n","              kernel_regularizer=regularizers.l2(0.01)))\n","    model.add(Dense(1, activation='sigmoid',  ))\n","\n","    model.compile(loss='binary_crossentropy',\n","              optimizer='rmsprop',\n","              metrics=['accuracy'])\n","\n","    train_datagen = ImageDataGenerator(rescale=1. / 255)\n","    test_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","    train_generator = train_datagen.flow_from_directory(\n","        train_data_dir,\n","        target_size=(img_width, img_height),#Como el resize\n","        batch_size=batch_size,\n","        class_mode='binary')\n","\n","    validation_generator = test_datagen.flow_from_directory(\n","        validation_data_dir,\n","        target_size=(img_width, img_height),\n","        batch_size=batch_size,\n","        class_mode='binary')\n","\n","    history = model.fit_generator(\n","        train_generator,\n","        steps_per_epoch=num_train_samples // batch_size,\n","        epochs=epochs,\n","        validation_data=validation_generator,\n","        validation_steps=num_validation_samples // batch_size, #Los generator sin ules infinitos que t dan valores, entonces de alg8una manera tiene  qsaber cuando cortar keras\n","        verbose =0)\n","    \n","    return history\n","\n","history = classification(num_train_samples = 4000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvXTvxnBhI7S","outputId":"1a8a367a-5e12-412c-a62f-94ffdc03751b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 368 images belonging to 6 classes.\n","Found 88 images belonging to 6 classes.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4000 batches). You may need to use the repeat() function when building your dataset.\n","WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 40 batches). You may need to use the repeat() function when building your dataset.\n"]}]},{"cell_type":"markdown","source":["#Usamos el data transfer en dos etapas"],"metadata":{"id":"yYo2-5DABg6s"}},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications import VGG16\n","\n","vgg16 = VGG16(weights='imagenet',# 'none' is random'imagenet' is optimized\n","              include_top = False, # Only the convolutional section\n","              input_shape = (150,150,3))\n","\n","train_data_dir = '/content/drive/MyDrive/Colab Notebooks/Inteligencia Artificial/Datasets/intel/train'\n","validation_data_dir = '/content/drive/MyDrive/Colab Notebooks/Inteligencia Artificial/Datasets/intel/validation'\n","    \n","def get_dogsCats_activations_fromVGG16(source_dir, num_features):\n","    activations = np.zeros((num_features, 4, 4, 512)) # block5_pool shape\n","    labels = np.zeros((num_features,))\n","    \n","    img_generator = ImageDataGenerator(rescale = 1./255.)\n","    batch_size = 32\n","    \n","    generator = img_generator.flow_from_directory(\n","        train_data_dir,\n","        target_size=(150, 150),\n","        batch_size=batch_size,\n","        class_mode='binary')\n","        \n","    i = num_features//batch_size - 1\n","    for input_batch, label_batch in generator:\n","        # each image enters in the vgg16 model\n","        output_batch = vgg16.predict(input_batch)\n","        # vgg16 exit activations   \n","        activations[i*batch_size:(i+1)*batch_size] = output_batch  \n","        labels[i*batch_size:(i+1)*batch_size] = label_batch # labels\n","        i -= 1\n","        if i == -1:\n","            break\n","            \n","    return activations, labels\n","    \n","train_activations, train_labels =get_dogsCats_activations_fromVGG16(train_data_dir,4096)\n","validation_activations, validation_labels = get_dogsCats_activations_fromVGG16(validation_data_dir,1024)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tkkxp34hApsl","outputId":"a69bb346-7c21-477c-887d-f42c8c212e6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 14038 images belonging to 6 classes.\n","1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Dropout\n","\n","model = Sequential()\n","model.add(Flatten())\n","# block5_pool shape (input_dim = 4*4*512) using 150x150 pixels\n","model.add(Dense(256, activation = 'relu', input_dim = 4*4*512)) \n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation = 'sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='rmsprop',\n","              metrics=['accuracy'])\n","\n","EPOCHS = 20\n","history = model.fit(train_activations, train_labels, epochs = EPOCHS,\n","                    batch_size = 32, validation_data =\n","                 (validation_activations, validation_labels), verbose = 0)\n","\n","plot(history)"],"metadata":{"id":"nKvHLcuGBK4R"},"execution_count":null,"outputs":[]}]}